# ******************************************************************************
# ******************************************************************************
#
#		Name : 		tokeniser.py
#		Purpose : 	RPL/32 Tokeniser Class
#		Author : 	Paul Robson (paul@robsons.org.uk)
#		Created : 	3rd October 2019
#
# ******************************************************************************
# ******************************************************************************

from tokens import *
import re,sys

# ******************************************************************************
#
#				Class which converts line to tokens
#
# ******************************************************************************

class Tokeniser(object):
	def __init__(self):
		self.tokens = RPLTokens().getTokens()								# get a list of tokens
		self.maxLen = 0														# find the length of longest key
		for t in self.tokens.keys():
			self.maxLen = max(self.maxLen,len(t))
	#
	#		Tokenise a string
	#
	def tokenise(self,s):
		self.code = []														# result code 
		s = s.replace("\n"," ").replace("\t"," ").strip()
		while s != "":														# tokenise everything
			s = self.tokeniseOne(s).strip()
		self.code.append(0)													# end of line marker
		return self.code
	#
	#		Tokenise a single item
	#
	def tokeniseOne(self,s):
		#
		if s.startswith('"') or s.startswith("'"):							# comment/string
			n = s[1:].find(s[0])											# find the end.
			assert n >= 0,"Unclosed string"
			self.code.append(1 if s[0] == '"' else 2)						# type length
			self.code.append(n+3)											# +3 for type,len,end
			for c in s[1:n+1]:												# characters
				self.code.append(ord(c))
			self.code.append(0)												# ASCIIZ
			return s[n+2:]
		#
		m = re.match("^(\\d+)(\\-?)(.*)$",s)								# constants
		if m is not None:
			self.compileNumber(int(m.group(1)) & 0xFFFFFFFF)				# get value and compile
			if m.group(2) == "-":											# if -ve output the
				self.code.append(self.tokens["{-}"])						# special constant -
			return m.group(3) 	
		#
		m = re.match("^([A-Za-z\\.]+)(.*)$",s)								# identifiers
		if m is not None and m.group(1).upper() not in self.tokens.keys()	:
			ident = m.group(1).upper()										# make upper case.
			ident = [self.convert(c) for c in ident]
			ident[-1] += 0x20 												# mark end of identifier
			self.code += ident
			return m.group(2)												# return rest of line
		#
		for l in range(self.maxLen,0,-1):									# check all lengths
			if s[:l].upper() in self.tokens: 								# found a match
				self.code.append(self.tokens[s[:l].upper()])				# add the token
				return s[l:] 												# remove from string
		#
		assert False,"Can't tokenise {"+s+"}"
	#
	#		Convert character to code
	#
	def convert(self,c):
		return 0xDF if c == '.' else 0xC0+ord(c)-ord('A')
	#
	#		Do one number (recursive)
	#
	def compileNumber(self,n):
		if n >= 64:
			self.compileNumber(n >> 6)
		self.code.append((n & 0x3F) + 0x80)
	#
	#		Test tokeniser
	#
	def test(self,s):
		code = self.tokenise(s)												# tokenise the code
		print("==== "+s+" ====")
		print("\t"+",".join(["${0:02x}".format(c) for c in code]))
		print()

if __name__ == "__main__":
	tok = Tokeniser()
	if len(sys.argv) == 2:
		print("; *** Generated by tokeniser.py ***")
		print("TestCode:")
		s = ",".join(["${0:02x}".format(c) for c in tok.tokenise(sys.argv[1])])
		print("\t.byte "+s)
		sys.exit(0)

	tok.test(" + @ ^ do until ")
	tok.test("46 74")
	tok.test(" 'hello' \"hello\" '' ")
	tok.test("zbc abc d0 fgha")
	tok.test("24576 ^prg prg0")
	tok.test("1 - 1-")
	tok.test("> >=")
	tok.test("hello.world")
	tok.test("defa ")
